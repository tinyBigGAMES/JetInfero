![JetInfero](media/jetinfero.png)  
[![Chat on Discord](https://img.shields.io/discord/754884471324672040?style=for-the-badge)](https://discord.gg/tPWjMwK)
[![Follow on Bluesky](https://img.shields.io/badge/Bluesky-tinyBigGAMES-blue?style=for-the-badge&logo=bluesky)](https://bsky.app/profile/tinybiggames.com)

## ğŸŒŸ Fast, Flexible Local LLM Inference for Developers ğŸš€

JetInfero is a nimble and high-performance library that enables developers to integrate local Large Language Models (LLMs) effortlessly into their applications. Powered by **llama.cpp** ğŸ•Šï¸, JetInfero prioritizes speed, flexibility, and ease of use ğŸŒ. Itâ€™s compatible with any language supporting **Win64**, **Unicode**, and dynamic-link libraries (DLLs).

## ğŸ’¡ Why Choose JetInfero?

- **Optimized for Speed** âš¡ï¸: Built on llama.cpp, JetInfero offers lightning-fast inference capabilities with minimal overhead.
- **Cross-Language Support** ğŸŒ: Seamlessly integrates with Delphi, C++, C#, Java, and other Win64-compatible environments.
- **Intuitive API** ğŸ”¬: A clean procedural API simplifies model management, inference execution, and callback handling.
- **Customizable Templates** ğŸ–‹ï¸: Tailor input prompts to suit different use cases with ease.
- **Scalable Performance** ğŸš€: Leverage GPU acceleration, token streaming, and multi-threaded execution for demanding workloads.

## ğŸ› ï¸ Key Features

### ğŸ¤– Advanced AI Integration

JetInfero expands your toolkit with capabilities such as:

- Dynamic chatbot creation ğŸ—£ï¸.
- Automated text generation ğŸ”„ and summarization ğŸ•».
- Context-aware content creation ğŸŒ.
- Real-time token streaming for adaptive applications âŒš.

### ğŸ”’ Privacy-Centric Local Execution

- Operates entirely offline ğŸ”, ensuring sensitive data remains secure.
- GPU acceleration supported via Vulkan for enhanced performance ğŸš’.

### âš™ï¸ Performance Optimization

- Configure GPU utilization with `AGPULayers` ğŸ”„.
- Allocate threads dynamically using `AMaxThreads` ğŸŒ.
- Access performance metrics to monitor throughput and efficiency ğŸ“Š.

### ğŸ”€ Flexible Prompt Templates

JetInferoâ€™s template system simplifies input customization. Templates include placeholders such as:

- **`{role}`**: Denotes the senderâ€™s role (e.g., `user`, `assistant`).
- **`{content}`**: Represents the message content.

For example:

```pascal
  jiDefineModel(
    // Model Filename
    'C:/LLM/GGUF/hermes-3-llama-3.2-3b-abliterated-q8_0.gguf', 
    
    // Model Refname
    'hermes-3-llama-3.2-3b-abliterated-q8_0',                  
    
     // Model Template
    '<|im_start|>system\n{content}<|im_end|>',                
    
     // Model Template End
    '<|im_start|>assistant\n',                                
    
    // Capitalize Role
    False,                                                     
    
    // Max Context
    8192,                                                      
    
    // Main GPU, -1 for best, 0..N GPU number
    -1,                                                        
    
    // GPU Layers, -1 for max, 0 for CPU only, 1..N for layer
    -1,                                                        
    
    // Max threads, default 4, max will be physical CPU count     
     4                                                         
  );
```

#### Template Benefits

- **Adaptability** ğŸŒ: Customize prompts for various LLMs and use cases.
- **Consistency** ğŸ”„: Ensure predictable inputs for reliable results.
- **Flexibility** ğŸŒˆ: Modify prompt formats for tasks like JSON or markdown generation.

### ğŸ‚ Streamlined Model Management

- Define models with `jiDefineModel` ğŸ”¨.
- Load/unload models dynamically using `jiLoadModel` and `jiUnloadModel` ğŸ”€.
- Save/load model configurations with `jiSaveModelDefines` and `jiLoadModelDefines` ğŸ—ƒï¸.
- Clear all model definitions using `jiClearModelDefines` ğŸ§¹.

### ğŸ” Inference Execution

- Perform inference tasks with `jiRunInference` âš™ï¸.
- Stream real-time tokens via `InferenceTokenCallback` âŒš.
- Retrieve responses using `jiGetInferenceResponse` ğŸ–Šï¸.

### ğŸ“Š Performance Monitoring

- Retrieve detailed metrics like tokens/second, input/output token counts, and execution time via `jiGetPerformanceResult` ğŸ“Š.

## ğŸ› ï¸ Installation

1. **Download the Repository** ğŸ“¦
   - [Download here](https://github.com/tinyBigGAMES/JetInfero/archive/refs/heads/main.zip) and extract the files to your preferred directory ğŸ“‚.

   Ensure `JetInfero.dll` is accessible in your project directory.

2. **Acquire a GGUF Model** ğŸ§ 
   - Obtain a model from [Hugging Face](https://huggingface.co), such as [
Hermes-3-Llama-3.2-3B-Q8_0-GGUF](https://huggingface.co/tinybiggames/Hermes-3-Llama-3.2-3B-Q8_0-GGUF/resolve/main/hermes-3-llama-3.2-3b-q8_0.gguf?download=true). Save it to a directory accessible to your application (e.g., `C:/LLM/GGUF`) ğŸ’¾.

2. **Add JetInfero to Your Project** ğŸ”¨
   - Include the `JetInfero` unit in your Delphi project.   

4. **Ensure GPU Compatibility** ğŸ®
   - Verify Vulkan compatibility for enhanced performance âš¡. Adjust `AGPULayers` as needed to accommodate VRAM limitations ğŸ“‰.
   
5. **Building JetInfero DLL** ğŸ› ï¸  
   - Open and compile the `JetInfero.dproj` project ğŸ“‚. This process will generate the 64-bit `JetInfero.dll` in the `lib` folder ğŸ—‚ï¸.  
   - The project was created and tested using Delphi 12.2 on Windows 11 24H2 ğŸ–¥ï¸.  

6. **Using JetInfero** ğŸš€  
   - JetInfero can be used with any programming language that supports Win64 and Unicode bindings ğŸ’».  
   - Ensure the `JetInfero.dll` is included in your distribution and accessible at runtime ğŸ“¦.  

## ğŸ“ˆ Quick Start

### âš™ï¸ Basic Setup

Integrate JetInfero into your Delphi project:

```pascal
uses
  JetInfero;

var
  LTokensPerSec: Double;
  LTotalInputTokens: Int32;
  LTotalOutputTokens: Int32;
begin
  if jiInit() then
  begin
    jiDefineModel(
      'C:/LLM/GGUF/hermes-3-llama-3.2-3b-abliterated-q8_0.gguf',
      'hermes-3-llama-3.2-3b-abliterated-q8_0',
      '<|im_start|>system\n{content}<|im_end|>',
      '<|im_start|>assistant\n', False, 8192, -1, -1, 4);
    
    jiLoadModel('hermes-3-llama-3.2-3b-abliterated-q8_0');

    jiAddMessage('user', 'What is AI?');

    if jiRunInference(PWideChar(LModelRef)) then
      begin
        jiGetPerformanceResult(@LTokensPerSec, @LTotalInputTokens, @LTotalOutputTokens);
        WriteLn('Input Tokens : ', LTotalInputTokens);
        WriteLn('Output Tokens: ', LTotalOutputTokens);
        WriteLn('Speed        : ', LTokensPerSec:3:2, ' t/s');
      end
    else
      begin
        WriteLn('Error: ', jiGetLastError());
      end;

    jiUnloadModel();
    jiQuit();
  end;

end.
```

### ğŸ” Using Callbacks

Define a custom callback to handle token streaming:

```pascal
procedure InferenceCallback(const Token: string; const UserData: Pointer);
begin
  Write(Token);
end;

jiSetTokenCallback(@InferenceCallback, nil);
```

### ğŸ“Š Retrieve Performance Metrics

Access performance results to monitor efficiency:

```pascal
var
  Metrics: TPerformanceResult;
begin
  Metrics := jiGetPerformanceResult();
  WriteLn('Tokens/Sec: ', Metrics.TokensPerSecond);
  WriteLn('Input Tokens: ', Metrics.TotalInputTokens);
  WriteLn('Output Tokens: ', Metrics.TotalOutputTokens);
end;
```

### ğŸ› ï¸ Support and Resources

- Report issues via the [Issue Tracker](https://github.com/tinyBigGAMES/jetinfero/issues) ğŸ.
- Engage in discussions on the [Forum](https://github.com/tinyBigGAMES/jetinfero/discussions) and [Discord](https://discord.gg/tPWjMwK) ğŸ’¬.
- Learn more at [Learn Delphi](https://learndelphi.org) ğŸ“š.

### ğŸ¤ Contributing  

Contributions to **âœ¨ JetInfero** are highly encouraged! ğŸŒŸ  
- ğŸ› **Report Issues:** Submit issues if you encounter bugs or need help.  
- ğŸ’¡ **Suggest Features:** Share your ideas to make **Lumina** even better.  
- ğŸ”§ **Create Pull Requests:** Help expand the capabilities and robustness of the library.  

Your contributions make a difference! ğŸ™Œâœ¨

#### Contributors ğŸ‘¥ğŸ¤
<br/>

<a href="https://github.com/tinyBigGAMES/JetInfero/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=tinyBigGAMES/JetInfero&max=500&columns=20&anon=1" />
</a>

### ğŸ“œ Licensing

**JetInfero** is distributed under the ğŸ†“ **BSD-3-Clause License**, allowing for redistribution and use in both source and binary forms, with or without modification, under specific conditions. See the [LICENSE](https://github.com/tinyBigGAMES/JetInfero?tab=BSD-3-Clause-1-ov-file#BSD-3-Clause-1-ov-file) file for more details.

---
**Elevate your Delphi projects with JetInfero ğŸš€ â€“ your bridge to seamless local generative AI integration ğŸ¤–.**  

<p align="center">
<img src="media/delphi.png" alt="Delphi">
</p>
<h5 align="center">

Made with :heart: in Delphi
